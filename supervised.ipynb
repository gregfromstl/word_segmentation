{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Implementation\n",
    "### Mandarin Word Segmentation Using BiLSTMs\n",
    "My supervised implementation is extremely similar to my \"Anything Goes\" implementation in `celtic_mutations`. Much of the code directly translated after getting the data preprocessing down and adjusting hyperparameters.\n",
    "I was able to achieve up to a 99% validation accuracy with 30% of the provided dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment variables. **Set `train_file` and `test_file` to the relative filepaths of the data.** If `test_file` is an empty string no test data will be used.\n",
    "The validation split determines the percentage of training samples set aside for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_file = \"data/train.tsv\"\n",
    "test_file = \"data/test.tsv\"\n",
    "val_split = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set random seed for reproducability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare the `TEXT` and `TAG` fields. In this implementation, the TAG field represent whether or not a character is the end of a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/student/rolwesg/.local/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "TEXT = data.Field(lower = True)\n",
    "TAGS = data.Field(unk_token = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fields = ((\"text\", TEXT), (\"tags\", TAGS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I again had to modify the `SequenceTaggingDataset` from torchtext. This time rather than specifying a character for a new example, I divided the examples into 500-character chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SequenceTaggingDataset(data.Dataset):\n",
    "    @staticmethod\n",
    "    def sort_key(example):\n",
    "        for attr in dir(example):\n",
    "            if not callable(getattr(example, attr)) and \\\n",
    "                    not attr.startswith(\"__\"):\n",
    "                return len(getattr(example, attr))\n",
    "        return 0\n",
    "\n",
    "    def __init__(self, path, fields, val_split=0, encoding=\"utf-8\", separator=\"\\t\", **kwargs):\n",
    "        print(\"Loading data...\")\n",
    "        examples = []\n",
    "        columns = []\n",
    "\n",
    "        with open(path, encoding=encoding) as input_file:\n",
    "            for idx, line in enumerate(input_file):\n",
    "                line = line.strip()\n",
    "                if columns and idx % 500 == 0:\n",
    "                    examples.append(data.Example.fromlist(columns, fields))\n",
    "                    columns = []\n",
    "                for i, column in enumerate(line.split(separator)):\n",
    "                    if len(columns) < i + 1:\n",
    "                        columns.append([])\n",
    "                    columns[i].append(column)\n",
    "            if columns:\n",
    "                examples.append(data.Example.fromlist(columns, fields))\n",
    "        print(\"Data loaded from {}\".format(path))\n",
    "        super(SequenceTaggingDataset, self).__init__(examples, fields,\n",
    "                                                     **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data into a Pytorch dataset and split based on the provided `val_split`. Load the test dataset if one is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/student/rolwesg/.local/lib/python3.7/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded from data/train.tsv\n",
      "Loading data...\n",
      "Data loaded from data/test.tsv\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data = SequenceTaggingDataset(train_file, fields).split(split_ratio=1-val_split)\n",
    "if len(test_file) > 0:\n",
    "    test_data = SequenceTaggingDataset(test_file, fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 11716\n",
      "Validation samples: 5021\n",
      "Testing samples: 396\n"
     ]
    }
   ],
   "source": [
    "print(\"Training samples: {}\".format(len(train_data)))\n",
    "print(\"Validation samples: {}\".format(len(val_data)))\n",
    "if \"test_data\" in globals():\n",
    "    print(\"Testing samples: {}\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['婆', '婆', '在', '長', '期', '的', '耳', '濡', '目', '染', '之', '下', '，', '也', '都', '是', '玩', '模', '型', '的', '高', '手', '。', '每', '當', '假', '日', '無', '處', '去', '時', '，', '全', '家', '陶', '醉', '在', '模', '型', '世', '界', '中', '，', '其', '樂', '融', '融', '。', '一', '種', '視', '覺', '上', '錯', '誤', '的', '反', '應', '現', '象', '，', '錯', '視', '早', '就', '被', '發', '現', '了', '，', '我', '們', '的', '眼', '睛', '受', '到', '環', '境', '的', '影', '響', '做', '出', '錯', '誤', '的', '判', '斷', '時', '，', '直', '線', '可', '能', '看', '成', '曲', '線', '，', '平', '行', '線', '可', '能', '看', '成', '歪', '斜', '線', '，', '失', '之', '毫', '釐', '，', '差', '以', '千', '里', '，', '有', '時', '錯', '的', '瘋', '狂', '，', '錯', '的', '離', '譜', '。', '大', '家', '常', '說', '眼', '見', '為', '憑', '，', '由', '於', '我', '們', '對', '眼', '睛', '的', '信', '賴', '程', '度', '，', '遠', '超', '過', '其', '他', '的', '知', '覺', '感', '觀', '，', '一', '旦', '看', '見', '與', '事', '實', '不', '相', '符', '的', '圖', '形', '時', '，', '第', '一', '個', '反', '應', '是', '不', '相', '信', '，', '非', '得', '以', '規', '矩', '實', '量', '才', '發', '現', '我', '們', '的', '靈', '魂', '之', '窗', '也', '有', '出', '錯', '的', '時', '候', '。', '藝', '術', '家', '們', '發', '現', '了', '錯', '視', '伴', '隨', '著', '奇', '趣', '的', '藝', '術', '效', '果', '，', '不', '斷', '有', '新', '的', '作', '品', '出', '現', '，', '看', '到', '這', '些', '，', '不', '得', '不', '佩', '服', '其', '巧', '運', '之', '匠', '心', '。', '錯', '視', '有', '：', '由', '知', '覺', '中', '樞', '所', '產', '生', '的', '心', '理', '錯', '視', '，', '由', '環', '境', '刺', '激', '所', '引', '起', '的', '物', '理', '錯', '視', '，', '及', '由', '感', '覺', '器', '官', '所', '產', '生', '的', '生', '理', '錯', '視', '。', '所', '謂', '心', '理', '錯', '覺', '是', '以', '我', '們', '生', '活', '上', '累', '積', '的', '經', '驗', '做', '判', '斷', '時', '所', '產', '生', '的', '，', '例', '如', '經', '驗', '告', '訴', '我', '們', '火', '是', '熱', '的', '，', '但', '事', '實', '上', '並', '不', '一', '定', '如', '此', '。', '物', '理', '錯', '覺', '指', '受', '到', '外', '在', '環', '境', '刺', '激', '所', '產', '生', '之', '錯', '覺', '，', '如', '靜', '止', '火', '車', '中', '之', '乘', '客', '看', '見', '旁', '邊', '開', '過', '之', '火', '車', '，', '會', '誤', '以', '為', '火', '車', '已', '往', '前', '開', '動', '，', '而', '由', '感', '覺', '器', '官', '所', '產', '生', '之', '生', '理', '錯', '覺', '，', '則', '是', '眼', '球', '本', '身', '受', '形', '色', '干', '擾', '所', '做', '之', '錯', '誤', '判', '斷', '，', '以', '下', '所', '列', '出', '之', '一', '些', '圖', '形', '大', '部', '份', '是', '屬', '於', '第', '三', '類', '。', '楊', '小', '雲', '８', '０', '年', '０', '９', '月', '１', '１', '日', '０', '７', '０', '３', '為', '台', '灣', '留', '下', '一', '塊', '乾', '淨', '的', '土', '地', '‧', '台', '塑', '企', '業', '的'], 'tags': ['0', '1', '1', '0', '1', '1', '0', '0', '0', '1', '0', '1', '1', '1', '1', '1', '1', '0', '1', '1', '0', '1', '1', '1', '1', '0', '1', '1', '1', '1', '1', '1', '1', '1', '0', '1', '1', '0', '1', '0', '1', '1', '1', '0', '0', '0', '1', '1', '1', '1', '0', '1', '1', '0', '1', '1', '0', '1', '0', '1', '1', '0', '1', '1', '1', '1', '0', '1', '1', '1', '0', '1', '1', '0', '1', '0', '1', '0', '1', '1', '0', '1', '0', '1', '0', '1', '1', '0', '1', '1', '1', '0', '1', '0', '1', '0', '1', '0', '1', '1', '0', '0', '1', '0', '1', '0', '1', '0', '1', '1', '1', '0', '0', '0', '1', '1', '0', '0', '0', '1', '1', '0', '1', '1', '1', '0', '1', '1', '1', '1', '0', '1', '1', '0', '1', '1', '1', '0', '0', '0', '1', '1', '0', '1', '0', '1', '1', '0', '1', '1', '0', '1', '0', '1', '1', '1', '0', '1', '0', '1', '1', '0', '1', '0', '1', '1', '0', '1', '0', '1', '1', '0', '1', '1', '0', '1', '1', '0', '1', '1', '1', '0', '1', '1', '0', '1', '1', '1', '0', '1', '1', '0', '1', '1', '0', '1', '1', '1', '1', '0', '1', '0', '1', '1', '0', '1', '1', '1', '1', '1', '0', '1', '1', '0', '1', '1', '0', '0', '1', '1', '0', '1', '1', '0', '1', '0', '1', '1', '0', '1', '1', '0', '1', '0', '1', '1', '0', '1', '1', '1', '1', '0', '1', '0', '1', '1', '0', '1', '0', '1', '1', '0', '0', '1', '0', '1', '1', '0', '1', '1', '0', '1', '1', '0', '1', '1', '1', '1', '0', '1', '0', '1', '1', '0', '1', '1', '0', '1', '0', '1', '1', '1', '0', '1', '0', '1', '1', '0', '1', '1', '0', '1', '0', '1', '1', '1', '1', '0', '1', '0', '1', '1', '0', '1', '1', '0', '1', '0', '1', '1', '0', '1', '0', '1', '0', '1', '1', '1', '0', '1', '0', '1', '1', '0', '1', '1', '0', '1', '1', '0', '1', '1', '1', '0', '1', '1', '1', '0', '1', '0', '1', '0', '1', '0', '1', '1', '1', '1', '1', '1', '1', '0', '1', '1', '1', '0', '0', '1', '0', '1', '1', '0', '1', '0', '1', '1', '0', '1', '0', '1', '0', '1', '0', '1', '1', '0', '1', '1', '0', '1', '1', '1', '0', '1', '0', '1', '1', '1', '0', '1', '0', '1', '0', '1', '1', '1', '1', '0', '1', '1', '1', '0', '0', '1', '0', '1', '1', '1', '1', '0', '1', '1', '1', '1', '0', '1', '0', '1', '1', '0', '1', '1', '0', '1', '0', '1', '1', '1', '1', '0', '1', '0', '1', '1', '0', '1', '0', '1', '1', '1', '1', '0', '1', '0', '1', '1', '0', '1', '1', '0', '1', '1', '0', '1', '0', '1', '0', '0', '1', '1', '0', '1', '0', '1', '1', '1', '0', '0', '1', '0', '0', '1', '0', '0', '1', '0', '0', '1', '0', '0', '0', '1', '1', '0', '1', '0', '1', '1', '1', '0', '1', '1', '0', '1', '1', '0', '1', '0', '1', '1']}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the vocab. I'm only including words that appear twice or more in the embeddings. Any unseen words or words with only one occurrence will be judged solely on the surrounding tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "MIN_FREQ = 2\n",
    "\n",
    "TEXT.build_vocab(train_data,\n",
    "                 min_freq = MIN_FREQ)\n",
    "TAGS.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number unique tokens in TEXT: 5454\n",
      "Unique tokens in TAG: ['<pad>', '1', '0']\n"
     ]
    }
   ],
   "source": [
    "print(\"Number unique tokens in TEXT: {}\".format(len(TEXT.vocab)))\n",
    "print(\"Unique tokens in TAG: {}\".format(TAGS.vocab.itos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the batch size and the GPU if one is available. **I was only able to run this in a reasonable amount of time using a GPU**.\n",
    "Then create the iterators to produce batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/student/rolwesg/.local/lib/python3.7/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "train_iterator, val_iterator = data.BucketIterator.splits(\n",
    "    (train_data, val_data),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device\n",
    ")\n",
    "if \"test_data\" in globals():\n",
    "    test_iterator = data.BucketIterator(test_data, batch_size = BATCH_SIZE, device = device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare the model class. I used the same model as the Celtic Mutations project. The only changes required were hyperparameter modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class WordSegmenter(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 embedding_dim,\n",
    "                 hidden_dim,\n",
    "                 output_dim,\n",
    "                 n_layers,\n",
    "                 bidirectional,\n",
    "                 dropout,\n",
    "                 pad_idx):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx = pad_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                            hidden_dim,\n",
    "                            num_layers = n_layers,\n",
    "                            bidirectional = bidirectional,\n",
    "                            dropout = dropout if n_layers > 1 else 0)\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        predictions = self.fc(self.dropout(outputs))\n",
    "\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100-dimensional embeddings, 4 bi-directional LSTMs, and 0.2 dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = len(TAGS.vocab)\n",
    "N_LAYERS = 4\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.2\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = WordSegmenter(INPUT_DIM,\n",
    "                        EMBEDDING_DIM,\n",
    "                        HIDDEN_DIM,\n",
    "                        OUTPUT_DIM,\n",
    "                        N_LAYERS,\n",
    "                        BIDIRECTIONAL,\n",
    "                        DROPOUT,\n",
    "                        PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I'm not using pretrained weights this time, initialize the embedding weights to have a Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordSegmenter(\n",
       "  (embedding): Embedding(5454, 100, padding_idx=1)\n",
       "  (lstm): LSTM(100, 128, num_layers=4, dropout=0.2, bidirectional=True)\n",
       "  (fc): Linear(in_features=256, out_features=3, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.normal_(param.data, mean = 0, std = 0.1)\n",
    "\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print trainable parameters to judge size of the model. It's fairly large, which explains the GPU requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1967483 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"{} trainable parameters\".format(count_parameters(model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set weights for padding to zero to ignore their affect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0130,  0.0206, -0.0029,  ..., -0.0323,  0.0617, -0.1056],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0989,  0.0110, -0.1453,  ...,  0.1078, -0.1063,  0.0542],\n",
      "        ...,\n",
      "        [-0.0893, -0.0824,  0.0827,  ...,  0.0522, -0.1137,  0.0026],\n",
      "        [-0.0141,  0.0566, -0.0076,  ..., -0.0563,  0.0193, -0.1662],\n",
      "        [-0.1331, -0.0218, -0.2010,  ..., -0.0623, -0.0537, -0.1236]])\n"
     ]
    }
   ],
   "source": [
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "print(model.embedding.weight.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard Adam optimizer with self-generated learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CrossEntropyLoss`, ignoring any outputs from padding tags since every word has an output, not just the whole sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "TAG_PAD_IDX = TAGS.vocab.stoi[TAGS.pad_token]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get index of positive labels. This will be used for calculating F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG_POS_IDX = TAGS.vocab.stoi['1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send the model and loss to the GPU is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss()"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine accuracy. This was pretty much a copy and paste from [this repo](https://github.com/bentrevett/pytorch-pos-tagging)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def categorical_accuracy(preds, y, tag_pad_idx):\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True)\n",
    "    non_pad_elements = (y != tag_pad_idx).nonzero()\n",
    "    correct = max_preds[non_pad_elements].squeeze(1).eq(y[non_pad_elements])\n",
    "    return correct.sum() / torch.FloatTensor([y[non_pad_elements].shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precision(preds, y, tag_pos_idx):\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True).squeeze(1)\n",
    "    pos_preds = (max_preds == tag_pos_idx).nonzero()\n",
    "    correct = max_preds[pos_preds].eq(y[pos_preds])\n",
    "    return correct.sum() / torch.FloatTensor([y[pos_preds].shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recall(preds, y, tag_pos_idx):\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True)\n",
    "    positives = (y == tag_pos_idx).nonzero()\n",
    "    correct = max_preds[positives].squeeze(1).eq(y[positives])\n",
    "    return correct.sum() / torch.FloatTensor([y[positives].shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1score(precision, recall):\n",
    "    return 2*((precision*recall)/(precision+recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard train and eval functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, tag_pad_idx, tag_pos_idx):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_precision = 0\n",
    "    epoch_recall = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch in iterator:\n",
    "        text = batch.text\n",
    "        tags = batch.tags\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        predictions = model(text.to(device))\n",
    "\n",
    "        # reshape predictions since pytorch can't handle 3-dimensional predictions\n",
    "        predictions = predictions.view(-1, predictions.shape[-1])\n",
    "        tags = tags.view(-1)\n",
    "\n",
    "        loss = criterion(predictions, tags.to(device))\n",
    "        \n",
    "        acc = categorical_accuracy(predictions.cpu(), tags.cpu(), tag_pad_idx)\n",
    "        precision = get_precision(predictions.cpu(), tags.cpu(), tag_pos_idx)\n",
    "        recall = get_recall(predictions.cpu(), tags.cpu(), tag_pos_idx)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        epoch_precision += precision.item()\n",
    "        epoch_recall += recall.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), f1score(epoch_precision / len(iterator), epoch_recall / len(iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion, tag_pad_idx, tag_pos_idx):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_precision = 0\n",
    "    epoch_recall = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text = batch.text\n",
    "            tags = batch.tags\n",
    "\n",
    "            predictions = model(text.to(device))\n",
    "\n",
    "            predictions = predictions.view(-1, predictions.shape[-1])\n",
    "            tags = tags.view(-1)\n",
    "\n",
    "            loss = criterion(predictions, tags.to(device))\n",
    "            acc = categorical_accuracy(predictions.cpu(), tags.cpu(), tag_pad_idx)\n",
    "            precision = get_precision(predictions.cpu(), tags.cpu(), tag_pos_idx)\n",
    "            recall = get_recall(predictions.cpu(), tags.cpu(), tag_pos_idx)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            epoch_precision += precision.item()\n",
    "            epoch_recall += recall.item()\n",
    "    \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), f1score(epoch_precision / len(iterator), epoch_recall / len(iterator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train for 100 epochs. This task took much longer than the Celtic Mutations task, but was able to reach similar accuracy eventually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.550 | Train Acc: 0.713 | Train F1-Score:  0.817\n",
      "Val Loss: 0.270 | Val Acc: 0.888 | Val F1-Score:  0.914\n",
      "Epoch: 2\n",
      "Train Loss: 0.238 | Train Acc: 0.903 | Train F1-Score:  0.926\n",
      "Val Loss: 0.205 | Val Acc: 0.919 | Val F1-Score:  0.937\n",
      "Epoch: 3\n",
      "Train Loss: 0.197 | Train Acc: 0.922 | Train F1-Score:  0.940\n",
      "Val Loss: 0.171 | Val Acc: 0.933 | Val F1-Score:  0.949\n",
      "Epoch: 4\n",
      "Train Loss: 0.171 | Train Acc: 0.933 | Train F1-Score:  0.949\n",
      "Val Loss: 0.151 | Val Acc: 0.942 | Val F1-Score:  0.955\n",
      "Epoch: 5\n",
      "Train Loss: 0.155 | Train Acc: 0.940 | Train F1-Score:  0.954\n",
      "Val Loss: 0.139 | Val Acc: 0.948 | Val F1-Score:  0.960\n",
      "Epoch: 6\n",
      "Train Loss: 0.143 | Train Acc: 0.946 | Train F1-Score:  0.958\n",
      "Val Loss: 0.128 | Val Acc: 0.952 | Val F1-Score:  0.963\n",
      "Epoch: 7\n",
      "Train Loss: 0.132 | Train Acc: 0.950 | Train F1-Score:  0.962\n",
      "Val Loss: 0.120 | Val Acc: 0.956 | Val F1-Score:  0.966\n",
      "Epoch: 8\n",
      "Train Loss: 0.124 | Train Acc: 0.954 | Train F1-Score:  0.964\n",
      "Val Loss: 0.113 | Val Acc: 0.959 | Val F1-Score:  0.968\n",
      "Epoch: 9\n",
      "Train Loss: 0.116 | Train Acc: 0.957 | Train F1-Score:  0.967\n",
      "Val Loss: 0.106 | Val Acc: 0.961 | Val F1-Score:  0.970\n",
      "Epoch: 10\n",
      "Train Loss: 0.110 | Train Acc: 0.959 | Train F1-Score:  0.969\n",
      "Val Loss: 0.103 | Val Acc: 0.963 | Val F1-Score:  0.971\n",
      "Epoch: 11\n",
      "Train Loss: 0.105 | Train Acc: 0.961 | Train F1-Score:  0.970\n",
      "Val Loss: 0.098 | Val Acc: 0.965 | Val F1-Score:  0.973\n",
      "Epoch: 12\n",
      "Train Loss: 0.101 | Train Acc: 0.963 | Train F1-Score:  0.972\n",
      "Val Loss: 0.094 | Val Acc: 0.966 | Val F1-Score:  0.974\n",
      "Epoch: 13\n",
      "Train Loss: 0.097 | Train Acc: 0.965 | Train F1-Score:  0.973\n",
      "Val Loss: 0.093 | Val Acc: 0.967 | Val F1-Score:  0.975\n",
      "Epoch: 14\n",
      "Train Loss: 0.093 | Train Acc: 0.966 | Train F1-Score:  0.974\n",
      "Val Loss: 0.089 | Val Acc: 0.968 | Val F1-Score:  0.976\n",
      "Epoch: 15\n",
      "Train Loss: 0.089 | Train Acc: 0.967 | Train F1-Score:  0.975\n",
      "Val Loss: 0.085 | Val Acc: 0.969 | Val F1-Score:  0.976\n",
      "Epoch: 16\n",
      "Train Loss: 0.084 | Train Acc: 0.969 | Train F1-Score:  0.976\n",
      "Val Loss: 0.081 | Val Acc: 0.971 | Val F1-Score:  0.977\n",
      "Epoch: 17\n",
      "Train Loss: 0.081 | Train Acc: 0.970 | Train F1-Score:  0.977\n",
      "Val Loss: 0.080 | Val Acc: 0.971 | Val F1-Score:  0.978\n",
      "Epoch: 18\n",
      "Train Loss: 0.078 | Train Acc: 0.971 | Train F1-Score:  0.978\n",
      "Val Loss: 0.077 | Val Acc: 0.972 | Val F1-Score:  0.978\n",
      "Epoch: 19\n",
      "Train Loss: 0.076 | Train Acc: 0.972 | Train F1-Score:  0.978\n",
      "Val Loss: 0.076 | Val Acc: 0.973 | Val F1-Score:  0.979\n",
      "Epoch: 20\n",
      "Train Loss: 0.074 | Train Acc: 0.973 | Train F1-Score:  0.979\n",
      "Val Loss: 0.074 | Val Acc: 0.973 | Val F1-Score:  0.979\n",
      "Epoch: 21\n",
      "Train Loss: 0.072 | Train Acc: 0.974 | Train F1-Score:  0.980\n",
      "Val Loss: 0.073 | Val Acc: 0.974 | Val F1-Score:  0.980\n",
      "Epoch: 22\n",
      "Train Loss: 0.070 | Train Acc: 0.974 | Train F1-Score:  0.980\n",
      "Val Loss: 0.071 | Val Acc: 0.974 | Val F1-Score:  0.980\n",
      "Epoch: 23\n",
      "Train Loss: 0.068 | Train Acc: 0.975 | Train F1-Score:  0.981\n",
      "Val Loss: 0.070 | Val Acc: 0.975 | Val F1-Score:  0.981\n",
      "Epoch: 24\n",
      "Train Loss: 0.067 | Train Acc: 0.975 | Train F1-Score:  0.981\n",
      "Val Loss: 0.069 | Val Acc: 0.975 | Val F1-Score:  0.981\n",
      "Epoch: 25\n",
      "Train Loss: 0.065 | Train Acc: 0.976 | Train F1-Score:  0.982\n",
      "Val Loss: 0.069 | Val Acc: 0.976 | Val F1-Score:  0.981\n",
      "Epoch: 26\n",
      "Train Loss: 0.064 | Train Acc: 0.977 | Train F1-Score:  0.982\n",
      "Val Loss: 0.068 | Val Acc: 0.976 | Val F1-Score:  0.981\n",
      "Epoch: 27\n",
      "Train Loss: 0.062 | Train Acc: 0.977 | Train F1-Score:  0.982\n",
      "Val Loss: 0.067 | Val Acc: 0.977 | Val F1-Score:  0.982\n",
      "Epoch: 28\n",
      "Train Loss: 0.061 | Train Acc: 0.978 | Train F1-Score:  0.983\n",
      "Val Loss: 0.066 | Val Acc: 0.977 | Val F1-Score:  0.982\n",
      "Epoch: 29\n",
      "Train Loss: 0.060 | Train Acc: 0.978 | Train F1-Score:  0.983\n",
      "Val Loss: 0.065 | Val Acc: 0.977 | Val F1-Score:  0.982\n",
      "Epoch: 30\n",
      "Train Loss: 0.059 | Train Acc: 0.978 | Train F1-Score:  0.983\n",
      "Val Loss: 0.065 | Val Acc: 0.977 | Val F1-Score:  0.982\n",
      "Epoch: 31\n",
      "Train Loss: 0.057 | Train Acc: 0.979 | Train F1-Score:  0.984\n",
      "Val Loss: 0.065 | Val Acc: 0.977 | Val F1-Score:  0.983\n",
      "Epoch: 32\n",
      "Train Loss: 0.056 | Train Acc: 0.979 | Train F1-Score:  0.984\n",
      "Val Loss: 0.063 | Val Acc: 0.978 | Val F1-Score:  0.983\n",
      "Epoch: 33\n",
      "Train Loss: 0.055 | Train Acc: 0.980 | Train F1-Score:  0.984\n",
      "Val Loss: 0.064 | Val Acc: 0.978 | Val F1-Score:  0.983\n",
      "Epoch: 34\n",
      "Train Loss: 0.055 | Train Acc: 0.980 | Train F1-Score:  0.985\n",
      "Val Loss: 0.063 | Val Acc: 0.978 | Val F1-Score:  0.983\n",
      "Epoch: 35\n",
      "Train Loss: 0.054 | Train Acc: 0.980 | Train F1-Score:  0.985\n",
      "Val Loss: 0.064 | Val Acc: 0.978 | Val F1-Score:  0.983\n",
      "Epoch: 36\n",
      "Train Loss: 0.053 | Train Acc: 0.981 | Train F1-Score:  0.985\n",
      "Val Loss: 0.061 | Val Acc: 0.979 | Val F1-Score:  0.984\n",
      "Epoch: 37\n",
      "Train Loss: 0.052 | Train Acc: 0.981 | Train F1-Score:  0.985\n",
      "Val Loss: 0.061 | Val Acc: 0.979 | Val F1-Score:  0.984\n",
      "Epoch: 38\n",
      "Train Loss: 0.051 | Train Acc: 0.981 | Train F1-Score:  0.986\n",
      "Val Loss: 0.062 | Val Acc: 0.979 | Val F1-Score:  0.984\n",
      "Epoch: 39\n",
      "Train Loss: 0.050 | Train Acc: 0.981 | Train F1-Score:  0.986\n",
      "Val Loss: 0.060 | Val Acc: 0.979 | Val F1-Score:  0.984\n",
      "Epoch: 40\n",
      "Train Loss: 0.050 | Train Acc: 0.982 | Train F1-Score:  0.986\n",
      "Val Loss: 0.062 | Val Acc: 0.979 | Val F1-Score:  0.984\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 40\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss, train_acc, train_f1 = train(model, train_iterator, optimizer, criterion, TAG_PAD_IDX, TAG_POS_IDX)\n",
    "    val_loss, val_acc, val_f1 = evaluate(model, val_iterator, criterion, TAG_PAD_IDX, TAG_POS_IDX)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'model.pt')\n",
    "\n",
    "    print(\"Epoch: {}\".format(epoch+1))\n",
    "    print(f\"Train Loss: {train_loss:.3f} | Train Acc: {train_acc:.3f} | Train F1-Score: {train_f1: .3f}\")\n",
    "    print(f\"Val Loss: {val_loss:.3f} | Val Acc: {val_acc:.3f} | Val F1-Score: {val_f1: .3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "The validation F1-Score exceeded 0.98, which seems pretty good, although as mentioned above it does take a number of iterations to hit its maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.109 | Test Acc: 0.962 | Test F1-Score:  0.969\n"
     ]
    }
   ],
   "source": [
    "if \"test_data\" in globals():\n",
    "    model.load_state_dict(torch.load('model.pt'))\n",
    "\n",
    "    test_loss, test_acc, test_f1 = evaluate(model, test_iterator, criterion, TAG_PAD_IDX, TAG_POS_IDX)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:.3f} | Test Acc: {test_acc:.3f} | Test F1-Score: {test_f1: .3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
