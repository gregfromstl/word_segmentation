{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi-Supervised Implementation\n",
    "### Mandarin Word Segmentation Using BiLSTMs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment variables. **Set `train_file` and `test_file` to the relative filepaths of the data.** If `test_file` is an empty string no test data will be used.\n",
    "The validation split determines the percentage of training samples set aside for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_file = \"data/train.tsv\"\n",
    "test_file = \"\"\n",
    "val_split = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set random seed for reproducability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare the `TEXT` and `TAG` fields. In this implementation, the TAG field represent whether or not a character is the end of a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/student/rolwesg/.local/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "TEXT = data.Field(lower = True)\n",
    "TAGS = data.Field(unk_token = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fields = ((\"text\", TEXT), (\"tags\", TAGS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I again had to modify the `SequenceTaggingDataset` from torchtext. This time rather than specifying a character for a new example, I divided the examples into 500-character chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SequenceTaggingDataset(data.Dataset):\n",
    "    @staticmethod\n",
    "    def sort_key(example):\n",
    "        for attr in dir(example):\n",
    "            if not callable(getattr(example, attr)) and \\\n",
    "                    not attr.startswith(\"__\"):\n",
    "                return len(getattr(example, attr))\n",
    "        return 0\n",
    "\n",
    "    def __init__(self, path, fields, val_split=0, encoding=\"utf-8\", separator=\"\\t\", **kwargs):\n",
    "        print(\"Loading data...\")\n",
    "        examples = []\n",
    "        columns = []\n",
    "\n",
    "        with open(path, encoding=encoding) as input_file:\n",
    "            for idx, line in enumerate(input_file):\n",
    "                line = line.strip()\n",
    "                if columns and idx % 500 == 0:\n",
    "                    examples.append(data.Example.fromlist(columns, fields))\n",
    "                    columns = []\n",
    "                for i, column in enumerate(line.split(separator)):\n",
    "                    if len(columns) < i + 1:\n",
    "                        columns.append([])\n",
    "                    columns[i].append(column)\n",
    "            if columns:\n",
    "                examples.append(data.Example.fromlist(columns, fields))\n",
    "        print(\"Data loaded from {}\".format(path))\n",
    "        super(SequenceTaggingDataset, self).__init__(examples, fields,\n",
    "                                                     **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data into a Pytorch dataset and split based on the provided `val_split`. Load the test dataset if one is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded from data/train.tsv\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data = SequenceTaggingDataset(train_file, fields).split(split_ratio=1-val_split)\n",
    "if len(test_file) > 0:\n",
    "    test_data = SequenceTaggingDataset(test_file, fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 11716\n",
      "Validation samples: 5021\n"
     ]
    }
   ],
   "source": [
    "print(\"Training samples: {}\".format(len(train_data)))\n",
    "print(\"Validation samples: {}\".format(len(val_data)))\n",
    "if \"test_data\" in globals():\n",
    "    print(\"Testing samples: {}\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['婆', '婆', '在', '長', '期', '的', '耳', '濡', '目', '染', '之', '下', '，', '也', '都', '是', '玩', '模', '型', '的', '高', '手', '。', '每', '當', '假', '日', '無', '處', '去', '時', '，', '全', '家', '陶', '醉', '在', '模', '型', '世', '界', '中', '，', '其', '樂', '融', '融', '。', '一', '種', '視', '覺', '上', '錯', '誤', '的', '反', '應', '現', '象', '，', '錯', '視', '早', '就', '被', '發', '現', '了', '，', '我', '們', '的', '眼', '睛', '受', '到', '環', '境', '的', '影', '響', '做', '出', '錯', '誤', '的', '判', '斷', '時', '，', '直', '線', '可', '能', '看', '成', '曲', '線', '，', '平', '行', '線', '可', '能', '看', '成', '歪', '斜', '線', '，', '失', '之', '毫', '釐', '，', '差', '以', '千', '里', '，', '有', '時', '錯', '的', '瘋', '狂', '，', '錯', '的', '離', '譜', '。', '大', '家', '常', '說', '眼', '見', '為', '憑', '，', '由', '於', '我', '們', '對', '眼', '睛', '的', '信', '賴', '程', '度', '，', '遠', '超', '過', '其', '他', '的', '知', '覺', '感', '觀', '，', '一', '旦', '看', '見', '與', '事', '實', '不', '相', '符', '的', '圖', '形', '時', '，', '第', '一', '個', '反', '應', '是', '不', '相', '信', '，', '非', '得', '以', '規', '矩', '實', '量', '才', '發', '現', '我', '們', '的', '靈', '魂', '之', '窗', '也', '有', '出', '錯', '的', '時', '候', '。', '藝', '術', '家', '們', '發', '現', '了', '錯', '視', '伴', '隨', '著', '奇', '趣', '的', '藝', '術', '效', '果', '，', '不', '斷', '有', '新', '的', '作', '品', '出', '現', '，', '看', '到', '這', '些', '，', '不', '得', '不', '佩', '服', '其', '巧', '運', '之', '匠', '心', '。', '錯', '視', '有', '：', '由', '知', '覺', '中', '樞', '所', '產', '生', '的', '心', '理', '錯', '視', '，', '由', '環', '境', '刺', '激', '所', '引', '起', '的', '物', '理', '錯', '視', '，', '及', '由', '感', '覺', '器', '官', '所', '產', '生', '的', '生', '理', '錯', '視', '。', '所', '謂', '心', '理', '錯', '覺', '是', '以', '我', '們', '生', '活', '上', '累', '積', '的', '經', '驗', '做', '判', '斷', '時', '所', '產', '生', '的', '，', '例', '如', '經', '驗', '告', '訴', '我', '們', '火', '是', '熱', '的', '，', '但', '事', '實', '上', '並', '不', '一', '定', '如', '此', '。', '物', '理', '錯', '覺', '指', '受', '到', '外', '在', '環', '境', '刺', '激', '所', '產', '生', '之', '錯', '覺', '，', '如', '靜', '止', '火', '車', '中', '之', '乘', '客', '看', '見', '旁', '邊', '開', '過', '之', '火', '車', '，', '會', '誤', '以', '為', '火', '車', '已', '往', '前', '開', '動', '，', '而', '由', '感', '覺', '器', '官', '所', '產', '生', '之', '生', '理', '錯', '覺', '，', '則', '是', '眼', '球', '本', '身', '受', '形', '色', '干', '擾', '所', '做', '之', '錯', '誤', '判', '斷', '，', '以', '下', '所', '列', '出', '之', '一', '些', '圖', '形', '大', '部', '份', '是', '屬', '於', '第', '三', '類', '。', '楊', '小', '雲', '８', '０', '年', '０', '９', '月', '１', '１', '日', '０', '７', '０', '３', '為', '台', '灣', '留', '下', '一', '塊', '乾', '淨', '的', '土', '地', '‧', '台', '塑', '企', '業', '的'], 'tags': ['0', '1', '1', '0', '1', '1', '0', '0', '0', '1', '0', '1', '1', '1', '1', '1', '1', '0', '1', '1', '0', '1', '1', '1', '1', '0', '1', '1', '1', '1', '1', '1', '1', '1', '0', '1', '1', '0', '1', '0', '1', '1', '1', '0', '0', '0', '1', '1', '1', '1', '0', '1', '1', '0', '1', '1', '0', '1', '0', '1', '1', '0', '1', '1', '1', '1', '0', '1', '1', '1', '0', '1', '1', '0', '1', '0', '1', '0', '1', '1', '0', '1', '0', '1', '0', '1', '1', '0', '1', '1', '1', '0', '1', '0', '1', '0', '1', '0', '1', '1', '0', '0', '1', '0', '1', '0', '1', '0', '1', '1', '1', '0', '0', '0', '1', '1', '0', '0', '0', '1', '1', '0', '1', '1', '1', '0', '1', '1', '1', '1', '0', '1', '1', '0', '1', '1', '1', '0', '0', '0', '1', '1', '0', '1', '0', '1', '1', '0', '1', '1', '0', '1', '0', '1', '1', '1', '0', '1', '0', '1', '1', '0', '1', '0', '1', '1', '0', '1', '0', '1', '1', '0', '1', '1', '0', '1', '1', '0', '1', '1', '1', '0', '1', '1', '0', '1', '1', '1', '0', '1', '1', '0', '1', '1', '0', '1', '1', '1', '1', '0', '1', '0', '1', '1', '0', '1', '1', '1', '1', '1', '0', '1', '1', '0', '1', '1', '0', '0', '1', '1', '0', '1', '1', '0', '1', '0', '1', '1', '0', '1', '1', '0', '1', '0', '1', '1', '0', '1', '1', '1', '1', '0', '1', '0', '1', '1', '0', '1', '0', '1', '1', '0', '0', '1', '0', '1', '1', '0', '1', '1', '0', '1', '1', '0', '1', '1', '1', '1', '0', '1', '0', '1', '1', '0', '1', '1', '0', '1', '0', '1', '1', '1', '0', '1', '0', '1', '1', '0', '1', '1', '0', '1', '0', '1', '1', '1', '1', '0', '1', '0', '1', '1', '0', '1', '1', '0', '1', '0', '1', '1', '0', '1', '0', '1', '0', '1', '1', '1', '0', '1', '0', '1', '1', '0', '1', '1', '0', '1', '1', '0', '1', '1', '1', '0', '1', '1', '1', '0', '1', '0', '1', '0', '1', '0', '1', '1', '1', '1', '1', '1', '1', '0', '1', '1', '1', '0', '0', '1', '0', '1', '1', '0', '1', '0', '1', '1', '0', '1', '0', '1', '0', '1', '0', '1', '1', '0', '1', '1', '0', '1', '1', '1', '0', '1', '0', '1', '1', '1', '0', '1', '0', '1', '0', '1', '1', '1', '1', '0', '1', '1', '1', '0', '0', '1', '0', '1', '1', '1', '1', '0', '1', '1', '1', '1', '0', '1', '0', '1', '1', '0', '1', '1', '0', '1', '0', '1', '1', '1', '1', '0', '1', '0', '1', '1', '0', '1', '0', '1', '1', '1', '1', '0', '1', '0', '1', '1', '0', '1', '1', '0', '1', '1', '0', '1', '0', '1', '0', '0', '1', '1', '0', '1', '0', '1', '1', '1', '0', '0', '1', '0', '0', '1', '0', '0', '1', '0', '0', '1', '0', '0', '0', '1', '1', '0', '1', '0', '1', '1', '1', '0', '1', '1', '0', '1', '1', '0', '1', '0', '1', '1']}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the vocab. I'm only including words that appear twice or more in the embeddings. Any unseen words or words with only one occurrence will be judged solely on the surrounding tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "MIN_FREQ = 2\n",
    "\n",
    "TEXT.build_vocab(train_data,\n",
    "                 min_freq = MIN_FREQ)\n",
    "TAGS.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number unique tokens in TEXT: 5454\n",
      "Unique tokens in TAG: ['<pad>', '1', '0']\n"
     ]
    }
   ],
   "source": [
    "print(\"Number unique tokens in TEXT: {}\".format(len(TEXT.vocab)))\n",
    "print(\"Unique tokens in TAG: {}\".format(TAGS.vocab.itos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the batch size and the GPU if one is available. **I was only able to run this in a reasonable amount of time using a GPU**.\n",
    "Then create the iterators to produce batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/student/rolwesg/.local/lib/python3.7/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "train_iterator, val_iterator = data.BucketIterator.splits(\n",
    "    (train_data, val_data),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device\n",
    ")\n",
    "if \"test_data\" in globals():\n",
    "    test_iterator = data.BucketIterator(test_data, batch_size = BATCH_SIZE, device = device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare the model class. I used the same model as the Celtic Mutations project. The only changes required were hyperparameter modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class WordSegmenter(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 embedding_dim,\n",
    "                 hidden_dim,\n",
    "                 output_dim,\n",
    "                 n_layers,\n",
    "                 bidirectional,\n",
    "                 dropout,\n",
    "                 pad_idx):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx = pad_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                            hidden_dim,\n",
    "                            num_layers = n_layers,\n",
    "                            bidirectional = bidirectional,\n",
    "                            dropout = dropout if n_layers > 1 else 0)\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        predictions = self.fc(self.dropout(outputs))\n",
    "\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = len(TAGS.vocab)\n",
    "N_LAYERS = 4\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.3\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = WordSegmenter(INPUT_DIM,\n",
    "                        EMBEDDING_DIM,\n",
    "                        HIDDEN_DIM,\n",
    "                        OUTPUT_DIM,\n",
    "                        N_LAYERS,\n",
    "                        BIDIRECTIONAL,\n",
    "                        DROPOUT,\n",
    "                        PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I'm not using pretrained weights this time, initialize the embedding weights to have a Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordSegmenter(\n",
       "  (embedding): Embedding(5454, 100, padding_idx=1)\n",
       "  (lstm): LSTM(100, 128, num_layers=4, dropout=0.3, bidirectional=True)\n",
       "  (fc): Linear(in_features=256, out_features=3, bias=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.normal_(param.data, mean = 0, std = 0.1)\n",
    "\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print trainable parameters to judge size of the model. It's fairly large, which explains the GPU requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1967483 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"{} trainable parameters\".format(count_parameters(model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set weights for padding to zero to ignore their affect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-5.7248e-02,  2.0565e-01,  5.2836e-02,  ...,  4.8831e-02,\n",
      "         -4.9211e-02,  1.1771e-01],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 9.4289e-02, -5.3422e-02, -1.1588e-01,  ...,  1.3952e-02,\n",
      "          1.6497e-01,  6.2637e-02],\n",
      "        ...,\n",
      "        [ 1.6150e-02,  7.4582e-02,  1.4738e-02,  ..., -1.5252e-01,\n",
      "         -7.4020e-02,  1.4197e-01],\n",
      "        [ 7.3277e-02,  3.1269e-02,  1.7995e-02,  ...,  1.6358e-01,\n",
      "          2.2453e-02,  1.3270e-01],\n",
      "        [-8.1434e-02,  7.5084e-02, -1.2482e-01,  ..., -1.0065e-04,\n",
      "          2.5365e-01,  8.0984e-02]])\n"
     ]
    }
   ],
   "source": [
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "print(model.embedding.weight.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard Adam optimizer with self-generated learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CrossEntropyLoss`, ignoring any outputs from padding tags since every word has an output, not just the whole sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "TAG_PAD_IDX = TAGS.vocab.stoi[TAGS.pad_token]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send the model and loss to the GPU is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss()"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine accuracy. This was pretty much a copy and paste from [this repo](https://github.com/bentrevett/pytorch-pos-tagging)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def categorical_accuracy(preds, y, tag_pad_idx):\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True)\n",
    "    non_pad_elements = (y != tag_pad_idx).nonzero()\n",
    "    correct = max_preds[non_pad_elements].squeeze(1).eq(y[non_pad_elements])\n",
    "    return correct.sum() / torch.FloatTensor([y[non_pad_elements].shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(preds, y, tag_pad_idx):\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True)\n",
    "    non_pad_elements = (y != tag_pad_idx).nonzero()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard train and eval functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, tag_pad_idx):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch in iterator:\n",
    "        text = batch.text\n",
    "        tags = batch.tags\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        predictions = model(text.to(device))\n",
    "\n",
    "        # reshape predictions since pytorch can't handle 3-dimensional predictions\n",
    "        predictions = predictions.view(-1, predictions.shape[-1])\n",
    "        tags = tags.view(-1)\n",
    "\n",
    "        loss = criterion(predictions, tags.to(device))\n",
    "\n",
    "        acc = categorical_accuracy(predictions.cpu(), tags.cpu(), tag_pad_idx)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion, tag_pad_idx):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text = batch.text\n",
    "            tags = batch.tags\n",
    "\n",
    "            predictions = model(text.to(device))\n",
    "\n",
    "            predictions = predictions.view(-1, predictions.shape[-1])\n",
    "            tags = tags.view(-1)\n",
    "\n",
    "            loss = criterion(predictions, tags.to(device))\n",
    "            acc = categorical_accuracy(predictions.cpu(), tags.cpu(), tag_pad_idx)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train for 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.568 | Train Acc: 0.699\n",
      "Val Loss: 0.299 | Val Acc: 0.874\n",
      "Epoch: 2\n",
      "Train Loss: 0.250 | Train Acc: 0.898\n",
      "Val Loss: 0.202 | Val Acc: 0.920\n",
      "Epoch: 3\n",
      "Train Loss: 0.200 | Train Acc: 0.921\n",
      "Val Loss: 0.173 | Val Acc: 0.934\n",
      "Epoch: 4\n",
      "Train Loss: 0.175 | Train Acc: 0.932\n",
      "Val Loss: 0.151 | Val Acc: 0.942\n",
      "Epoch: 5\n",
      "Train Loss: 0.159 | Train Acc: 0.939\n",
      "Val Loss: 0.141 | Val Acc: 0.947\n",
      "Epoch: 6\n",
      "Train Loss: 0.147 | Train Acc: 0.944\n",
      "Val Loss: 0.131 | Val Acc: 0.951\n",
      "Epoch: 7\n",
      "Train Loss: 0.138 | Train Acc: 0.948\n",
      "Val Loss: 0.123 | Val Acc: 0.955\n",
      "Epoch: 8\n",
      "Train Loss: 0.130 | Train Acc: 0.951\n",
      "Val Loss: 0.116 | Val Acc: 0.957\n",
      "Epoch: 9\n",
      "Train Loss: 0.123 | Train Acc: 0.954\n",
      "Val Loss: 0.112 | Val Acc: 0.960\n",
      "Epoch: 10\n",
      "Train Loss: 0.118 | Train Acc: 0.956\n",
      "Val Loss: 0.106 | Val Acc: 0.961\n",
      "Epoch: 11\n",
      "Train Loss: 0.112 | Train Acc: 0.958\n",
      "Val Loss: 0.103 | Val Acc: 0.963\n",
      "Epoch: 12\n",
      "Train Loss: 0.108 | Train Acc: 0.960\n",
      "Val Loss: 0.098 | Val Acc: 0.964\n",
      "Epoch: 13\n",
      "Train Loss: 0.104 | Train Acc: 0.962\n",
      "Val Loss: 0.096 | Val Acc: 0.966\n",
      "Epoch: 14\n",
      "Train Loss: 0.101 | Train Acc: 0.963\n",
      "Val Loss: 0.095 | Val Acc: 0.967\n",
      "Epoch: 15\n",
      "Train Loss: 0.098 | Train Acc: 0.964\n",
      "Val Loss: 0.090 | Val Acc: 0.968\n",
      "Epoch: 16\n",
      "Train Loss: 0.095 | Train Acc: 0.965\n",
      "Val Loss: 0.088 | Val Acc: 0.969\n",
      "Epoch: 17\n",
      "Train Loss: 0.091 | Train Acc: 0.966\n",
      "Val Loss: 0.086 | Val Acc: 0.969\n",
      "Epoch: 18\n",
      "Train Loss: 0.088 | Train Acc: 0.967\n",
      "Val Loss: 0.082 | Val Acc: 0.970\n",
      "Epoch: 19\n",
      "Train Loss: 0.084 | Train Acc: 0.968\n",
      "Val Loss: 0.079 | Val Acc: 0.971\n",
      "Epoch: 20\n",
      "Train Loss: 0.082 | Train Acc: 0.969\n",
      "Val Loss: 0.079 | Val Acc: 0.972\n",
      "Epoch: 21\n",
      "Train Loss: 0.080 | Train Acc: 0.970\n",
      "Val Loss: 0.076 | Val Acc: 0.973\n",
      "Epoch: 22\n",
      "Train Loss: 0.079 | Train Acc: 0.971\n",
      "Val Loss: 0.074 | Val Acc: 0.973\n",
      "Epoch: 23\n",
      "Train Loss: 0.077 | Train Acc: 0.972\n",
      "Val Loss: 0.074 | Val Acc: 0.974\n",
      "Epoch: 24\n",
      "Train Loss: 0.075 | Train Acc: 0.972\n",
      "Val Loss: 0.073 | Val Acc: 0.974\n",
      "Epoch: 25\n",
      "Train Loss: 0.073 | Train Acc: 0.973\n",
      "Val Loss: 0.072 | Val Acc: 0.974\n",
      "Epoch: 26\n",
      "Train Loss: 0.072 | Train Acc: 0.973\n",
      "Val Loss: 0.071 | Val Acc: 0.975\n",
      "Epoch: 27\n",
      "Train Loss: 0.071 | Train Acc: 0.974\n",
      "Val Loss: 0.072 | Val Acc: 0.975\n",
      "Epoch: 28\n",
      "Train Loss: 0.070 | Train Acc: 0.974\n",
      "Val Loss: 0.070 | Val Acc: 0.975\n",
      "Epoch: 29\n",
      "Train Loss: 0.068 | Train Acc: 0.975\n",
      "Val Loss: 0.069 | Val Acc: 0.976\n",
      "Epoch: 30\n",
      "Train Loss: 0.067 | Train Acc: 0.975\n",
      "Val Loss: 0.069 | Val Acc: 0.976\n",
      "Epoch: 31\n",
      "Train Loss: 0.066 | Train Acc: 0.976\n",
      "Val Loss: 0.067 | Val Acc: 0.976\n",
      "Epoch: 32\n",
      "Train Loss: 0.065 | Train Acc: 0.976\n",
      "Val Loss: 0.067 | Val Acc: 0.977\n",
      "Epoch: 33\n",
      "Train Loss: 0.064 | Train Acc: 0.976\n",
      "Val Loss: 0.067 | Val Acc: 0.977\n",
      "Epoch: 34\n",
      "Train Loss: 0.063 | Train Acc: 0.977\n",
      "Val Loss: 0.065 | Val Acc: 0.977\n",
      "Epoch: 35\n",
      "Train Loss: 0.062 | Train Acc: 0.977\n",
      "Val Loss: 0.066 | Val Acc: 0.977\n",
      "Epoch: 36\n",
      "Train Loss: 0.062 | Train Acc: 0.977\n",
      "Val Loss: 0.065 | Val Acc: 0.977\n",
      "Epoch: 37\n",
      "Train Loss: 0.061 | Train Acc: 0.978\n",
      "Val Loss: 0.063 | Val Acc: 0.978\n",
      "Epoch: 38\n",
      "Train Loss: 0.060 | Train Acc: 0.978\n",
      "Val Loss: 0.064 | Val Acc: 0.978\n",
      "Epoch: 39\n",
      "Train Loss: 0.059 | Train Acc: 0.978\n",
      "Val Loss: 0.063 | Val Acc: 0.978\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 100\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, TAG_PAD_IDX)\n",
    "    val_loss, val_acc = evaluate(model, val_iterator, criterion, TAG_PAD_IDX)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'model.pt')\n",
    "\n",
    "    print(\"Epoch: {}\".format(epoch+1))\n",
    "    print(f\"Train Loss: {train_loss:.3f} | Train Acc: {train_acc:.3f}\")\n",
    "    print(f\"Val Loss: {val_loss:.3f} | Val Acc: {val_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"test_data\" in globals():\n",
    "    model.load_state_dict(torch.load('model.pt'))\n",
    "\n",
    "    test_loss, test_data = evaluate(model, test_iterator, criterion, TAG_PAD_IDX)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:.3f} | Test Acc: {tes_acc:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
